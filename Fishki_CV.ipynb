{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc13c790",
      "metadata": {},
      "source": [
        "# Нейросеть для сегментации изображений"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2632de97",
      "metadata": {},
      "source": [
        "### Входные данные"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6d9a2c",
      "metadata": {},
      "source": [
        "•\tДатасет в папке `fishki_labelme`:\n",
        "- набор из 140 изображений (2448x2448x3 JPG);\n",
        "- файлы разметки в формате `.json` из [labelme](https://github.com/wkentaro/labelme);\n",
        "- файл `obj.names` с именами объектов/классов:\n",
        "-- __background__ - фоновые пиксели;\n",
        "-- fishka - пиксели области фишки;\n",
        "-- defect - пиксели области дефекта.\n",
        "\n",
        "•\tСкрипт `01_generate_dataset.py` - генерирует датасет для обучения НС в формате [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/). Он также разбивает выборку на train (95%) и val и test (5%). Последние 2 выборки одинаковые (val=test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b533b9ea",
      "metadata": {},
      "source": [
        "### Задачи"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce8d463",
      "metadata": {},
      "source": [
        "**Задача №1:** Подготовка датасета\n",
        "Скрипт `01_generate_dataset.py` нужно модифицировать (или написать свой) чтобы он разбивал исходный набор отдельно на train(80%) val(10%) и test(10%).\n",
        "Выходными данными должны являться:\n",
        "•\tГотовый к обучению датасет в формате Pascal VOC; \n",
        "\n",
        "**Задача №2:** Обучение НС\n",
        "Необходимо обучить НС сегментатора на датасете из задачи №1\n",
        "Фреймворк машинного обучения и библиотеки можно использовать любые по желанию. Необходимо обосновать выбор.\n",
        "Входное разрешение нейросети при обучении необходимо также выбрать и обосновать.\n",
        "Выходными данными должны являться \n",
        "•\tобученная нейросеть сегментации с train датасетом из задачи №1;\n",
        "•\tлог обучения (графики функции потерь и mIoU от эпохи);\n",
        "•\tрасчет метрик по сегментации на val датасете (IoU по каждому классу отдельно и mIoU);\n",
        "•\tрасчет метрик по сегментации на test датасете (IoU по каждому классу отдельно и mIoU);\n",
        "\n",
        "**Задача №3:** Инференс НС\n",
        "Необходимо прогнать изображения из тестового датасета через обученную в задаче №2 нейросеть сегментатора и получить визуализации.\n",
        "При выполнении задания можно использовать средства фреймворка машинного обучения (PyTorch, Tensorflow), либо сконвертировать обученную НС в формат ONNX.\n",
        "\n",
        "Выходными данными должны являться\n",
        "•\tизображения из полученного в задаче №1 test датасета размеченные обученной в задаче №2 нейросетью.\n",
        "\n",
        "По результату тестового задания должен быть представлен краткий отчет с описанием выполненных работ, результатов тестирования НС и примерами изображений размеченных нейросетью.\n",
        "\n",
        "Примечание: в данном случае для примера сделана просто визуализация разметки, вы должны будете сделать раскраску по результатам сегментации входных изображений нейросетью.\n",
        "\n",
        "Задача сводится к классификации каждого пикселя. Необзодимо определить к какому объекту (классу) он пренадлежит. После, выделить на изображении части одного объекта, т.е. сегментировать изображение. Задача обучения сводится к минимизации функции ошибки на этапе классификации пикселей."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coCWqLxU2lMo",
      "metadata": {
        "id": "coCWqLxU2lMo"
      },
      "source": [
        "## Подключаем необходимые модули"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179",
      "metadata": {
        "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is OFF\n"
          ]
        }
      ],
      "source": [
        "# испорт основных библиотек\n",
        "import os\n",
        "#import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# импорт спец. библиотек и функций\n",
        "import pydot\n",
        "#import graphviz\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "from skimage import measure\n",
        "from skimage.io import imread, imsave, imshow\n",
        "from skimage.transform import resize\n",
        "from skimage.filters import gaussian\n",
        "from skimage.morphology import dilation, disk\n",
        "from skimage.draw import polygon, polygon_perimeter\n",
        "\n",
        "# проверка наличия GPU-ускорителя\n",
        "print(f'GPU is {\"ON\" if tf.config.list_physical_devices(\"GPU\") else \"OFF\" }')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3",
      "metadata": {
        "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3"
      },
      "source": [
        "## Подготовим набор данных для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "82be98ae-bc6e-4d49-b205-db5130879caa",
      "metadata": {
        "id": "82be98ae-bc6e-4d49-b205-db5130879caa"
      },
      "outputs": [],
      "source": [
        "CLASSES = 2 # кол-во классов + один класс обозначающий задний план\n",
        "COLORS = ['black', 'red', 'green'] # цветовое обозначение классов\n",
        "\n",
        "SAMPLE_SIZE = (256, 256) # размер входного изображения для НС\n",
        "OUTPUT_SIZE = (2448, 2448) # разммер изображения на выходе НС"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "500caf45-f2b9-48af-8f91-7d31d44ec266",
      "metadata": {
        "id": "500caf45-f2b9-48af-8f91-7d31d44ec266"
      },
      "outputs": [],
      "source": [
        "# функция загрузки и преобразования фото и маски\n",
        "def load_images(image, mask):\n",
        "    image = tf.io.read_file(image) # чтение фото\n",
        "    image = tf.io.decode_jpeg(image)\n",
        "    image = tf.image.resize(image, OUTPUT_SIZE) # стандартизация размера\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = image / 255.0 # нормализация фото (отмасштабировали будущие признаки)\n",
        "\n",
        "    # аналогичная операция выполняется для маски\n",
        "    mask = tf.io.read_file(mask)\n",
        "    mask = tf.io.decode_png(mask)\n",
        "    mask = tf.image.resize(mask, OUTPUT_SIZE)\n",
        "    mask = tf.image.convert_image_dtype(mask, tf.float32)\n",
        "    mask = mask / 128.0\n",
        "    mask = mask[:, :, 0:1] # отбрасываю Blue channel из-за неинформативности\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e591bcb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# функция аунментация фото и маски, соответственно\n",
        "def augmentate_images(image, masks):\n",
        "    # увеличение масштаба на случайную величину\n",
        "    random_crop = tf.random.uniform((), 0.8, 1)\n",
        "    image = tf.image.central_crop(image, random_crop)\n",
        "    masks = tf.image.central_crop(masks, random_crop)\n",
        "\n",
        "    # отражение по горизонтали\n",
        "    random_flip = tf.random.uniform((), 0, 1)\n",
        "    if random_flip >= 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        masks = tf.image.flip_left_right(masks)\n",
        "\n",
        "    # назвачение входного размера фото и маски\n",
        "    image = tf.image.resize(image, SAMPLE_SIZE)\n",
        "    masks = tf.image.resize(masks, SAMPLE_SIZE)\n",
        "\n",
        "    return image, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dcf6424",
      "metadata": {},
      "outputs": [],
      "source": [
        "# загрузка имён фото и соответстующих из масок\n",
        "def get_image_names(sample):\n",
        "    images = []\n",
        "    \n",
        "    file = open('fishki_voc_dataset/ImageSets/Segmentation/' + sample + '.txt', 'r')\n",
        "    for line in file:\n",
        "        images.append('fishki_voc_dataset/JPEGImages/'       + line[:-2] + '.jpg')\n",
        "    file.close()\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def get_masks_names(sample):\n",
        "    masks  = []\n",
        "    \n",
        "    file = open('fishki_voc_dataset/ImageSets/Segmentation/'+ sample    +'.txt', 'r')\n",
        "    for line in file:\n",
        "        masks.append('fishki_voc_dataset/SegmentationClass/'+ line[:-2] +'.png')\n",
        "    file.close()\n",
        "\n",
        "    return masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f6151ff4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fishki_voc_dataset/JPEGImages/0000010.jpg', 'fishki_voc_dataset/JPEGImages/0000002.jpg', 'fishki_voc_dataset/JPEGImages/0000010.jpg', 'fishki_voc_dataset/JPEGImages/0000011.jpg', 'fishki_voc_dataset/JPEGImages/0000010.jpg', 'fishki_voc_dataset/JPEGImages/0000006.jpg', 'fishki_voc_dataset/JPEGImages/0000002.jpg', 'fishki_voc_dataset/JPEGImages/0000006.jpg', 'fishki_voc_dataset/JPEGImages/0000005.jpg', 'fishki_voc_dataset/JPEGImages/0000001.jpg', 'fishki_voc_dataset/JPEGImages/0000007.jpg', 'fishki_voc_dataset/JPEGImages/0000006.jpg', 'fishki_voc_dataset/JPEGImages/0000004.jpg', 'fishki_voc_dataset/JPEGImages/0000001.jpg']\n"
          ]
        }
      ],
      "source": [
        "# загрузка имён фото и соответстующих из масок\n",
        "#images = sorted(glob.glob('fishki_voc_dataset/JPEGImages/*.jpg'))\n",
        "#masks  = sorted(glob.glob('fishki_voc_dataset/SegmentationClass/*.png'))\n",
        "\n",
        "# формаривание наборов данных\n",
        "images_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "masks_dataset  = tf.data.Dataset.from_tensor_slices(masks)\n",
        "dataset = tf.data.Dataset.zip((images_dataset, masks_dataset))\n",
        "\n",
        "dataset = dataset.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)       # загрузка данных в память с помощью функции load_images\n",
        "dataset = dataset.repeat(60)                                                  # копирование датасета в памяти N раз\n",
        "dataset = dataset.map(augmentate_images, num_parallel_calls=tf.data.AUTOTUNE) # аугментация датасета с помощью функции augmentate_images\n",
        "\n",
        "print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "86faac58",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_ParallelMapDataset element_spec=(TensorSpec(shape=(256, 256, None), dtype=tf.float32, name=None), TensorSpec(shape=(256, 256, None), dtype=tf.float32, name=None))>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "754a86cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fishki_voc_dataset/JPEGImages/0000010.jpg', 'fishki_voc_dataset/JPEGImages/0000002.jpg', 'fishki_voc_dataset/JPEGImages/0000010.jpg', 'fishki_voc_dataset/JPEGImages/0000011.jpg', 'fishki_voc_dataset/JPEGImages/0000010.jpg', 'fishki_voc_dataset/JPEGImages/0000006.jpg', 'fishki_voc_dataset/JPEGImages/0000002.jpg', 'fishki_voc_dataset/JPEGImages/0000006.jpg', 'fishki_voc_dataset/JPEGImages/0000005.jpg', 'fishki_voc_dataset/JPEGImages/0000001.jpg', 'fishki_voc_dataset/JPEGImages/0000007.jpg', 'fishki_voc_dataset/JPEGImages/0000006.jpg', 'fishki_voc_dataset/JPEGImages/0000004.jpg', 'fishki_voc_dataset/JPEGImages/0000001.jpg']\n"
          ]
        }
      ],
      "source": [
        "print(get_image_names('test'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923",
      "metadata": {
        "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923"
      },
      "source": [
        "## Посмотрим на содержимое набора данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2",
      "metadata": {
        "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "images_and_masks = list(dataset.take(5))\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize=(15, 5), dpi=125)\n",
        "\n",
        "for i, (image, masks) in enumerate(images_and_masks):\n",
        "    ax[0, i].set_title('Image')\n",
        "    ax[0, i].set_axis_off()\n",
        "    ax[0, i].imshow(image)\n",
        "\n",
        "    ax[1, i].set_title('Mask')\n",
        "    ax[1, i].set_axis_off()\n",
        "    ax[1, i].imshow(image)\n",
        "\n",
        "    for channel in range(CLASSES):\n",
        "        contours = measure.find_contours(np.array(masks[:,:,channel]))\n",
        "        for contour in contours:\n",
        "            ax[1, i].plot(contour[:, 1], contour[:, 0], linewidth=1, color=COLORS[channel])\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a2c4bf-74e0-41de-86c6-e89687ef3ca4",
      "metadata": {
        "id": "a5a2c4bf-74e0-41de-86c6-e89687ef3ca4"
      },
      "source": [
        "## Разделим набор данных на обучающий и проверочный"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd9e3f67-5f19-48f3-9a9f-09660b64d49a",
      "metadata": {
        "id": "cd9e3f67-5f19-48f3-9a9f-09660b64d49a"
      },
      "outputs": [],
      "source": [
        "# выделение тренировочной и тестовой выборки + кэширование\n",
        "train_dataset = dataset.take(2000).cache()\n",
        "test_dataset = dataset.skip(2000).take(100).cache()\n",
        "\n",
        "# установка размера частей (батчей) применяемых для обучения\n",
        "train_dataset = train_dataset.batch(16)\n",
        "test_dataset = test_dataset.batch(16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482b3f41-5324-41e1-944d-809ec06ee959",
      "metadata": {
        "id": "482b3f41-5324-41e1-944d-809ec06ee959"
      },
      "source": [
        "## Обозначим основные блоки модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b",
      "metadata": {
        "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b"
      },
      "outputs": [],
      "source": [
        "def input_layer():\n",
        "    return tf.keras.layers.Input(shape=SAMPLE_SIZE + (3,))\n",
        "\n",
        "\n",
        "def downsample_block(filters, size, batch_norm=True):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    model.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if batch_norm:\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    return model\n",
        "\n",
        "\n",
        "def upsample_block(filters, size, dropout=False):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if dropout:\n",
        "        model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.ReLU())\n",
        "    return model\n",
        "\n",
        "\n",
        "def output_layer(size):\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    return tf.keras.layers.Conv2DTranspose(CLASSES, size, strides=2, padding='same', kernel_initializer=initializer, activation='sigmoid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc",
      "metadata": {
        "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc"
      },
      "source": [
        "## Построим U-NET подобную архитектуру"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c4c326c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "inp_layer = input_layer()\n",
        "\n",
        "downsample_stack = [\n",
        "    downsample_block(64, 4, batch_norm=False),\n",
        "    downsample_block(128, 4),\n",
        "    downsample_block(256, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "]\n",
        "\n",
        "upsample_stack = [\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(256, 4),\n",
        "    upsample_block(128, 4),\n",
        "    upsample_block(64, 4)\n",
        "]\n",
        "\n",
        "out_layer = output_layer(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40",
      "metadata": {
        "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "# добавление skip-connections связей\n",
        "x = inp_layer\n",
        "\n",
        "downsample_skips = []\n",
        "\n",
        "for block in downsample_stack:\n",
        "    x = block(x)\n",
        "    downsample_skips.append(x)\n",
        "\n",
        "downsample_skips = reversed(downsample_skips[:-1])\n",
        "\n",
        "for up_block, down_block in zip(upsample_stack, downsample_skips):\n",
        "    x = up_block(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, down_block])\n",
        "\n",
        "out_layer = out_layer(x)\n",
        "\n",
        "unet_like = tf.keras.Model(inputs=inp_layer, outputs=out_layer)\n",
        "\n",
        "tf.keras.utils.plot_model(unet_like, show_shapes=True, dpi=72)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "be7ab218",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(unet_like, show_shapes=True, dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67f732f-120b-49a8-bc7c-304709f12db5",
      "metadata": {
        "id": "b67f732f-120b-49a8-bc7c-304709f12db5"
      },
      "source": [
        "## Определим метрики и функции потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381",
      "metadata": {
        "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381"
      },
      "outputs": [],
      "source": [
        "def dice_mc_metric(a, b):\n",
        "    a = tf.unstack(a, axis=3)\n",
        "    b = tf.unstack(b, axis=3)\n",
        "\n",
        "    dice_summ = 0\n",
        "\n",
        "    for i, (aa, bb) in enumerate(zip(a, b)):\n",
        "        numenator = 2 * tf.math.reduce_sum(aa * bb) + 1\n",
        "        denomerator = tf.math.reduce_sum(aa + bb) + 1\n",
        "        dice_summ += numenator / denomerator\n",
        "\n",
        "    avg_dice = dice_summ / CLASSES\n",
        "\n",
        "    return avg_dice\n",
        "\n",
        "def dice_mc_loss(a, b):\n",
        "    return 1 - dice_mc_metric(a, b)\n",
        "\n",
        "def dice_bce_mc_loss(a, b):\n",
        "    return 0.3 * dice_mc_loss(a, b) + tf.keras.losses.binary_crossentropy(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c",
      "metadata": {
        "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c"
      },
      "source": [
        "## Компилируем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1",
      "metadata": {
        "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1"
      },
      "outputs": [],
      "source": [
        "unet_like.compile(optimizer='adam', loss=[dice_bce_mc_loss], metrics=[dice_mc_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bfd329-31b4-4200-8282-1cb066d52b83",
      "metadata": {
        "id": "75bfd329-31b4-4200-8282-1cb066d52b83"
      },
      "source": [
        "## Обучаем нейронную сеть и сохраняем результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb",
      "metadata": {
        "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_dice \u001b[39m=\u001b[39m unet_like\u001b[39m.\u001b[39mfit(train_dataset, validation_data\u001b[39m=\u001b[39mtest_dataset, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, initial_epoch\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39m#unet_like.save_weights('SemanticSegmentationNetworks/unet_like')\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "history_dice = unet_like.fit(train_dataset, validation_data=test_dataset, epochs=1, initial_epoch=0)\n",
        "\n",
        "#unet_like.save_weights('SemanticSegmentationNetworks/unet_like')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311e395e-5465-4507-a75c-7ea95dc19e69",
      "metadata": {
        "id": "311e395e-5465-4507-a75c-7ea95dc19e69"
      },
      "source": [
        "## Загрузим модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c",
      "metadata": {
        "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c"
      },
      "outputs": [],
      "source": [
        "unet_like.load_weights('SemanticSegmentationLesson/networks/unet_like')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e978c692-4136-419b-9365-e5fbf98bbf50",
      "metadata": {
        "id": "e978c692-4136-419b-9365-e5fbf98bbf50"
      },
      "source": [
        "## Проверим работу сети на всех кадрах из видео"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c",
      "metadata": {
        "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c"
      },
      "outputs": [],
      "source": [
        "rgb_colors = [\n",
        "    (0,   0,   0),\n",
        "    (255, 0,   0),\n",
        "    (0,   255, 0),\n",
        "    (0,   0,   255),\n",
        "    (255, 165, 0),\n",
        "    (255, 192, 203),\n",
        "    (0,   255, 255),\n",
        "    (255, 0,   255)\n",
        "]\n",
        "\n",
        "frames = sorted(glob.glob('SemanticSegmentationLesson/videos/original_video/*.jpg'))\n",
        "\n",
        "for filename in frames:\n",
        "    frame = imread(filename)\n",
        "    sample = resize(frame, SAMPLE_SIZE)\n",
        "\n",
        "    predict = unet_like.predict(sample.reshape((1,) +  SAMPLE_SIZE + (3,)))\n",
        "    predict = predict.reshape(SAMPLE_SIZE + (CLASSES,))\n",
        "\n",
        "    scale = frame.shape[0] / SAMPLE_SIZE[0], frame.shape[1] / SAMPLE_SIZE[1]\n",
        "\n",
        "    frame = (frame / 1.5).astype(np.uint8)\n",
        "\n",
        "    for channel in range(1, CLASSES):\n",
        "        contour_overlay = np.zeros((frame.shape[0], frame.shape[1]))\n",
        "        contours = measure.find_contours(np.array(predict[:,:,channel]))\n",
        "\n",
        "        try:\n",
        "            for contour in contours:\n",
        "                rr, cc = polygon_perimeter(contour[:, 0] * scale[0],\n",
        "                                           contour[:, 1] * scale[1],\n",
        "                                           shape=contour_overlay.shape)\n",
        "\n",
        "                contour_overlay[rr, cc] = 1\n",
        "\n",
        "            contour_overlay = dilation(contour_overlay, disk(1))\n",
        "            frame[contour_overlay == 1] = rgb_colors[channel]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    imsave(f'SemanticSegmentationLesson/videos/processed/{os.path.basename(filename)}', frame)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
