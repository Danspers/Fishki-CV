{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc13c790",
      "metadata": {},
      "source": [
        "# Нейросеть для сегментации изображений"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0dc31ab",
      "metadata": {},
      "source": [
        "## Описание проекта"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047bc1da",
      "metadata": {},
      "source": [
        "Задача проекта сводится к классификации каждого пикселя. Необходимо определить к какому объекту (классу) он принадлежит. После, выделить на изображении части принадлежащие одному объекту, т.е. сегментировать изображение.\n",
        "Задача обучения сводится к минимизации функции ошибки на этапе классификации пикселей."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2632de97",
      "metadata": {},
      "source": [
        "### Входные данные"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6d9a2c",
      "metadata": {},
      "source": [
        "•\tДатасет в папке `fishki_labelme`:\n",
        "- набор из 140 изображений (2448x2448x3 JPG);\n",
        "- файлы разметки в формате `.json` из [labelme](https://github.com/wkentaro/labelme);\n",
        "- файл `obj.names` с именами объектов/классов:\n",
        "    - __background__ - фоновые пиксели;\n",
        "    - fishka - пиксели области фишки;\n",
        "    - defect - пиксели области дефекта.\n",
        "\n",
        "•\tСкрипт `01_generate_dataset.py` - генерирует датасет для обучения НС в формате [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/). Он также разбивает выборку на train (95%) и val и test (5%). Последние 2 выборки одинаковые (val=test)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b533b9ea",
      "metadata": {},
      "source": [
        "### Задачи"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce8d463",
      "metadata": {},
      "source": [
        "**Задача №1:** _Подготовка датасета_\n",
        "\n",
        "Скрипт `01_generate_dataset.py` нужно модифицировать (или написать свой) чтобы он разбивал исходный набор отдельно на train(80%) val(10%) и test(10%).\n",
        "Выходными данными должны являться:\n",
        "Готовый к обучению датасет в формате Pascal VOC; \n",
        "\n",
        "**Задача №2:** _Обучение НС_\n",
        "\n",
        "Необходимо обучить НС сегментатора на датасете из задачи №1\n",
        "Фреймворк машинного обучения и библиотеки можно использовать любые по желанию. Необходимо обосновать выбор.\n",
        "Входное разрешение нейросети при обучении необходимо также выбрать и обосновать.\n",
        "\n",
        "Выходными данными должны являться \n",
        "-\tобученная нейросеть сегментации с train датасетом из задачи №1;\n",
        "-\tлог обучения (графики функции потерь и mIoU от эпохи);\n",
        "-\tрасчет метрик по сегментации на val датасете (IoU по каждому классу отдельно и mIoU);\n",
        "-\tрасчет метрик по сегментации на test датасете (IoU по каждому классу отдельно и mIoU);\n",
        "\n",
        "**Задача №3:** _Инференс НС_\n",
        "\n",
        "Необходимо прогнать изображения из тестового датасета через обученную в задаче №2 нейросеть сегментатора и получить визуализации.\n",
        "При выполнении задания можно использовать средства фреймворка машинного обучения (`PyTorch`, `Tensorflow`), либо сконвертировать обученную НС в формат ONNX.\n",
        "\n",
        "Выходными данными должны являться изображения из полученного в задаче №1 test датасета размеченные обученной в задаче №2 нейросетью."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54fd1bc7",
      "metadata": {},
      "source": [
        "По результату тестового задания должен быть представлен краткий отчет с описанием выполненных работ, результатов тестирования НС и примерами изображений размеченных нейросетью.\n",
        "\n",
        "Примечание: в данном случае для примера сделана просто визуализация разметки, вы должны будете сделать раскраску по результатам сегментации входных изображений нейросетью.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coCWqLxU2lMo",
      "metadata": {
        "id": "coCWqLxU2lMo"
      },
      "source": [
        "## Подключаем необходимые модули"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179",
      "metadata": {
        "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179"
      },
      "outputs": [],
      "source": [
        "# импорт основных библиотек\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# импорт спец. библиотек и функций\n",
        "import tensorflow as tf\n",
        "from skimage import measure\n",
        "from skimage.io import imread, imsave, imshow\n",
        "from skimage.transform import resize\n",
        "from skimage.filters import gaussian\n",
        "from skimage.morphology import dilation, disk\n",
        "from skimage.draw import polygon, polygon_perimeter\n",
        "\n",
        "# проверка наличия GPU-ускорителя\n",
        "print(f'GPU is {\"ON\" if tf.config.list_physical_devices(\"GPU\") else \"OFF\" }')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3",
      "metadata": {
        "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3"
      },
      "source": [
        "## Подготовим набор данных для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82be98ae-bc6e-4d49-b205-db5130879caa",
      "metadata": {
        "id": "82be98ae-bc6e-4d49-b205-db5130879caa"
      },
      "outputs": [],
      "source": [
        "CLASSES = 3 # кол-во классов + один класс обозначающий задний план\n",
        "COLORS = ['black', 'red', 'green'] # цветовое обозначение классов\n",
        "\n",
        "SAMPLE_SIZE = (256, 256) # размер входного изображения для НС\n",
        "OUTPUT_SIZE = (2448, 2448) # размер изображения на выходе НС"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500caf45-f2b9-48af-8f91-7d31d44ec266",
      "metadata": {
        "id": "500caf45-f2b9-48af-8f91-7d31d44ec266"
      },
      "outputs": [],
      "source": [
        "# функция загрузки и преобразования фото и маски\n",
        "def load_images(image, mask):\n",
        "    image = tf.io.read_file(image) # чтение фото\n",
        "    image = tf.io.decode_jpeg(image)\n",
        "    image = tf.image.resize(image, OUTPUT_SIZE)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = image / 255.0 # нормализация фото (отмасштабировали будущие признаки)\n",
        "\n",
        "    # аналогичная операция выполняется для маски\n",
        "    mask = tf.io.read_file(mask)\n",
        "    mask = tf.io.decode_png(mask)\n",
        "    mask = tf.image.rgb_to_grayscale(mask)\n",
        "    mask = tf.image.resize(mask, OUTPUT_SIZE)\n",
        "    mask = tf.image.convert_image_dtype(mask, tf.float32)\n",
        "\n",
        "    masks = []\n",
        "    uniq_values, uniq_id = tf.unique(tf.reshape(mask, [-1]))\n",
        "\n",
        "    for tone in [ 0., 38., 75.]:\n",
        "        masks.append(tf.where(tf.equal(mask, tone), 1.0, 0.0))\n",
        "    \n",
        "    masks = tf.stack(masks, axis=2)\n",
        "    masks = tf.reshape(masks, OUTPUT_SIZE + (CLASSES,))\n",
        "\n",
        "    return image, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4f05cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "mask = 'fishki_voc_dataset/SegmentationClass/00000001.png'\n",
        "mask = tf.io.read_file(mask)\n",
        "mask = tf.io.decode_png(mask)\n",
        "mask = tf.image.rgb_to_grayscale(mask)\n",
        "mask = tf.image.resize(mask, OUTPUT_SIZE)\n",
        "mask = tf.image.convert_image_dtype(mask, tf.float32)\n",
        "\n",
        "masks = []\n",
        "uniq_values, uniq_id = tf.unique(tf.reshape(mask, [-1]))\n",
        "print(uniq_values)\n",
        "\n",
        "for tone in [ 0., 38., 75.]:\n",
        "    masks.append(tf.where(tf.equal(mask, tone), 1.0, 0.0))\n",
        "    print(tf.where(tf.equal(mask, tone), 1.0, 0.0))    \n",
        "\n",
        "masks = tf.stack(masks, axis=2)\n",
        "masks = tf.reshape(masks, OUTPUT_SIZE + (CLASSES,))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec18930",
      "metadata": {},
      "outputs": [],
      "source": [
        "#plt.imshow(mask)\n",
        "fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 5), dpi=125)\n",
        "ax[0].imshow(masks[:, :, 0])\n",
        "ax[0].set_axis_off()\n",
        "ax[1].imshow(masks[:, :, 1])\n",
        "ax[1].set_axis_off()\n",
        "ax[2].imshow(masks[:, :, 2])\n",
        "ax[2].set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e591bcb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# функция аугментация фото и маски, соответственно\n",
        "def augmentate_images(image, masks):\n",
        "    # увеличение масштаба на случайную величину\n",
        "    random_crop = tf.random.uniform((), 0.8, 1)\n",
        "    image = tf.image.central_crop(image, random_crop)\n",
        "    masks = tf.image.central_crop(masks, random_crop)\n",
        "\n",
        "    # отражение по горизонтали\n",
        "    random_flip = tf.random.uniform((), 0, 1)\n",
        "    if random_flip >= 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        masks = tf.image.flip_left_right(masks)\n",
        "\n",
        "    # назначение входного размера фото и маски\n",
        "    image = tf.image.resize(image, SAMPLE_SIZE)\n",
        "    masks = tf.image.resize(masks, SAMPLE_SIZE)\n",
        "\n",
        "    return image, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dcf6424",
      "metadata": {},
      "outputs": [],
      "source": [
        "# загрузка имён фото и соответствующих масок\n",
        "def get_image_dataset(sample):\n",
        "    images = []\n",
        "    masks  = []\n",
        "    \n",
        "    # загрузка имён фото и соответствующих масок\n",
        "    file = open('fishki_voc_dataset/ImageSets/Segmentation/' + sample + '.txt', 'r')\n",
        "    for line in file:\n",
        "        images.append('fishki_voc_dataset/JPEGImages\\\\' + line[:-1] + '.jpg')\n",
        "        masks.append('fishki_voc_dataset/SegmentationClass\\\\'+ line[:-1] +'.png')\n",
        "    file.close()\n",
        "\n",
        "    # формирование набора данных\n",
        "    images_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "    masks_dataset  = tf.data.Dataset.from_tensor_slices(masks)\n",
        "    dataset = tf.data.Dataset.zip((images_dataset, masks_dataset))\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6151ff4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# загрузка датасетов\n",
        "train = get_image_dataset('train')\n",
        "train = train.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)       # загрузка данных в память с помощью функции load_images\n",
        "#train = train.repeat(60)                                                  # копирование датасета в памяти N раз\n",
        "train = train.map(augmentate_images, num_parallel_calls=tf.data.AUTOTUNE) # аугментация датасета с помощью функции augmentate_images\n",
        "train = train.batch(16)\n",
        "\n",
        "valid = get_image_dataset('val')\n",
        "valid = valid.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid = valid.map(augmentate_images, num_parallel_calls=tf.data.AUTOTUNE) # аугментация датасета с помощью функции augmentate_images\n",
        "valid = valid.batch(16)\n",
        "\n",
        "test = get_image_dataset('test')\n",
        "test = test.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test = test.batch(16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923",
      "metadata": {
        "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923"
      },
      "source": [
        "## Посмотрим на содержимое набора данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2",
      "metadata": {
        "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "images_and_masks = list(train.take(5))\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize=(15, 5), dpi=125)\n",
        "\n",
        "for i, (image, masks) in enumerate(images_and_masks):\n",
        "    ax[0, i].set_title('Image')\n",
        "    ax[0, i].set_axis_off()\n",
        "    ax[0, i].imshow(image)\n",
        "\n",
        "    ax[1, i].set_title('Mask')\n",
        "    ax[1, i].set_axis_off()\n",
        "    ax[1, i].imshow(image)\n",
        "\n",
        "    for channel in range(CLASSES):\n",
        "        contours = measure.find_contours(np.array(masks[:, :, channel]))\n",
        "        for contour in contours:\n",
        "            ax[1, i].plot(contour[:, 1], contour[:, 0], linewidth=1, color=COLORS[channel])\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482b3f41-5324-41e1-944d-809ec06ee959",
      "metadata": {
        "id": "482b3f41-5324-41e1-944d-809ec06ee959"
      },
      "source": [
        "## Обозначим основные блоки модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b",
      "metadata": {
        "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b"
      },
      "outputs": [],
      "source": [
        "def input_layer():\n",
        "    return tf.keras.layers.Input(shape=SAMPLE_SIZE + (3,))\n",
        "\n",
        "\n",
        "def downsample_block(filters, size, batch_norm=True):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    model.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if batch_norm:\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    return model\n",
        "\n",
        "\n",
        "def upsample_block(filters, size, dropout=False):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if dropout:\n",
        "        model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.ReLU())\n",
        "    return model\n",
        "\n",
        "\n",
        "def output_layer(size):\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    return tf.keras.layers.Conv2DTranspose(CLASSES, size, strides=2, padding='same', kernel_initializer=initializer, activation='sigmoid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc",
      "metadata": {
        "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc"
      },
      "source": [
        "## Построим U-NET подобную архитектуру"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c326c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "inp_layer = input_layer()\n",
        "\n",
        "downsample_stack = [\n",
        "    downsample_block(64, 4, batch_norm=False),\n",
        "    downsample_block(128, 4),\n",
        "    downsample_block(256, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "]\n",
        "\n",
        "upsample_stack = [\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(256, 4),\n",
        "    upsample_block(128, 4),\n",
        "    upsample_block(64, 4)\n",
        "]\n",
        "\n",
        "out_layer = output_layer(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40",
      "metadata": {
        "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40"
      },
      "outputs": [],
      "source": [
        "# добавление skip-connections связей\n",
        "x = inp_layer\n",
        "\n",
        "downsample_skips = []\n",
        "\n",
        "for block in downsample_stack:\n",
        "    x = block(x)\n",
        "    downsample_skips.append(x)\n",
        "\n",
        "downsample_skips = reversed(downsample_skips[:-1])\n",
        "\n",
        "for up_block, down_block in zip(upsample_stack, downsample_skips):\n",
        "    x = up_block(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, down_block])\n",
        "\n",
        "out_layer = out_layer(x)\n",
        "\n",
        "unet_like = tf.keras.Model(inputs=inp_layer, outputs=out_layer)\n",
        "\n",
        "tf.keras.utils.plot_model(unet_like, show_shapes=True, dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67f732f-120b-49a8-bc7c-304709f12db5",
      "metadata": {
        "id": "b67f732f-120b-49a8-bc7c-304709f12db5"
      },
      "source": [
        "## Определим метрики и функции потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381",
      "metadata": {
        "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381"
      },
      "outputs": [],
      "source": [
        "def dice_mc_metric(a, b):\n",
        "    a = tf.unstack(a, axis=3)\n",
        "    b = tf.unstack(b, axis=3)\n",
        "\n",
        "    dice_summ = 0\n",
        "\n",
        "    for i, (aa, bb) in enumerate(zip(a, b)):\n",
        "        numenator = 2 * tf.math.reduce_sum(aa * bb) + 1\n",
        "        denomerator = tf.math.reduce_sum(aa + bb) + 1\n",
        "        dice_summ += numenator / denomerator\n",
        "\n",
        "    avg_dice = dice_summ / CLASSES\n",
        "\n",
        "    return avg_dice\n",
        "\n",
        "def dice_mc_loss(a, b):\n",
        "    return 1 - dice_mc_metric(a, b)\n",
        "\n",
        "def dice_bce_mc_loss(a, b):\n",
        "    return 0.3 * dice_mc_loss(a, b) + tf.keras.losses.binary_crossentropy(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c",
      "metadata": {
        "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c"
      },
      "source": [
        "## Компилируем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1",
      "metadata": {
        "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1"
      },
      "outputs": [],
      "source": [
        "unet_like.compile(optimizer='adam', loss=[tf.keras.losses.BinaryCrossentropy], metrics=['iou']) # tf.keras.losses.BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bfd329-31b4-4200-8282-1cb066d52b83",
      "metadata": {
        "id": "75bfd329-31b4-4200-8282-1cb066d52b83"
      },
      "source": [
        "## Обучаем нейронную сеть и сохраняем результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb",
      "metadata": {
        "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb"
      },
      "outputs": [],
      "source": [
        "history_dice = unet_like.fit(train, validation_data=valid, epochs=2, initial_epoch=0)\n",
        "\n",
        "#unet_like.save_weights('SemanticSegmentationNetworks/unet_like')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311e395e-5465-4507-a75c-7ea95dc19e69",
      "metadata": {
        "id": "311e395e-5465-4507-a75c-7ea95dc19e69"
      },
      "source": [
        "## Загрузим модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c",
      "metadata": {
        "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c"
      },
      "outputs": [],
      "source": [
        "unet_like.load_weights('SemanticSegmentationLesson/networks/unet_like')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e978c692-4136-419b-9365-e5fbf98bbf50",
      "metadata": {
        "id": "e978c692-4136-419b-9365-e5fbf98bbf50"
      },
      "source": [
        "## Проверим работу сети на всех кадрах из видео"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c",
      "metadata": {
        "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c"
      },
      "outputs": [],
      "source": [
        "rgb_colors = [\n",
        "    (0,   0,   0),\n",
        "    (255, 0,   0),\n",
        "    (0,   255, 0),\n",
        "    (0,   0,   255),\n",
        "    (255, 165, 0),\n",
        "    (255, 192, 203),\n",
        "    (0,   255, 255),\n",
        "    (255, 0,   255)\n",
        "]\n",
        "\n",
        "frames = sorted(glob.glob('SemanticSegmentationLesson/videos/original_video/*.jpg'))\n",
        "\n",
        "for filename in frames:\n",
        "    frame = imread(filename)\n",
        "    sample = resize(frame, SAMPLE_SIZE)\n",
        "\n",
        "    predict = unet_like.predict(sample.reshape((1,) +  SAMPLE_SIZE + (3,)))\n",
        "    predict = predict.reshape(SAMPLE_SIZE + (CLASSES,))\n",
        "\n",
        "    scale = frame.shape[0] / SAMPLE_SIZE[0], frame.shape[1] / SAMPLE_SIZE[1]\n",
        "\n",
        "    frame = (frame / 1.5).astype(np.uint8)\n",
        "\n",
        "    for channel in range(1, CLASSES):\n",
        "        contour_overlay = np.zeros((frame.shape[0], frame.shape[1]))\n",
        "        contours = measure.find_contours(np.array(predict[:,:,channel]))\n",
        "\n",
        "        try:\n",
        "            for contour in contours:\n",
        "                rr, cc = polygon_perimeter(contour[:, 0] * scale[0],\n",
        "                                           contour[:, 1] * scale[1],\n",
        "                                           shape=contour_overlay.shape)\n",
        "\n",
        "                contour_overlay[rr, cc] = 1\n",
        "\n",
        "            contour_overlay = dilation(contour_overlay, disk(1))\n",
        "            frame[contour_overlay == 1] = rgb_colors[channel]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    imsave(f'SemanticSegmentationLesson/videos/processed/{os.path.basename(filename)}', frame)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
