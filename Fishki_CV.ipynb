{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc13c790",
      "metadata": {},
      "source": [
        "# Нейросеть для сегментации изображений"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0dc31ab",
      "metadata": {},
      "source": [
        "## Описание проекта"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047bc1da",
      "metadata": {},
      "source": [
        "Задача проекта сводится к классификации каждого пикселя. Необходимо определить к какому объекту (классу) он принадлежит. После, выделить на изображении части принадлежащие одному объекту, т.е. сегментировать изображение.\n",
        "Задача обучения сводится к минимизации функции ошибки на этапе классификации пикселей."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2632de97",
      "metadata": {},
      "source": [
        "### Входные данные"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6d9a2c",
      "metadata": {},
      "source": [
        "•\tДатасет в папке `fishki_labelme`:\n",
        "- набор из 140 изображений (2448x2448x3 JPG);\n",
        "- файлы разметки в формате `.json` из [labelme](https://github.com/wkentaro/labelme);\n",
        "- файл `obj.names` с именами объектов/классов:\n",
        "    - __background__ - фоновые пиксели;\n",
        "    - fishka - пиксели области фишки;\n",
        "    - defect - пиксели области дефекта.\n",
        "\n",
        "•\tСкрипт `01_generate_dataset.py` - генерирует датасет для обучения НС в формате [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/). Он также разбивает выборку на train (95%) и val и test (5%). Последние 2 выборки одинаковые (val=test)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b533b9ea",
      "metadata": {},
      "source": [
        "### Задачи"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce8d463",
      "metadata": {},
      "source": [
        "**Задача №1:** _Подготовка датасета_\n",
        "\n",
        "Скрипт `01_generate_dataset.py` нужно модифицировать (или написать свой) чтобы он разбивал исходный набор отдельно на train(80%) val(10%) и test(10%).\n",
        "Выходными данными должны являться:\n",
        "Готовый к обучению датасет в формате Pascal VOC; \n",
        "\n",
        "**Задача №2:** _Обучение НС_\n",
        "\n",
        "Необходимо обучить НС сегментатора на датасете из задачи №1\n",
        "Фреймворк машинного обучения и библиотеки можно использовать любые по желанию. Необходимо обосновать выбор.\n",
        "Входное разрешение нейросети при обучении необходимо также выбрать и обосновать.\n",
        "\n",
        "Выходными данными должны являться \n",
        "-\tобученная нейросеть сегментации с train датасетом из задачи №1;\n",
        "-\tлог обучения (графики функции потерь и mIoU от эпохи);\n",
        "-\tрасчет метрик по сегментации на val датасете (IoU по каждому классу отдельно и mIoU);\n",
        "-\tрасчет метрик по сегментации на test датасете (IoU по каждому классу отдельно и mIoU);\n",
        "\n",
        "**Задача №3:** _Инференс НС_\n",
        "\n",
        "Необходимо прогнать изображения из тестового датасета через обученную в задаче №2 нейросеть сегментатора и получить визуализации.\n",
        "При выполнении задания можно использовать средства фреймворка машинного обучения (`PyTorch`, `Tensorflow`), либо сконвертировать обученную НС в формат ONNX.\n",
        "\n",
        "Выходными данными должны являться изображения из полученного в задаче №1 test датасета размеченные обученной в задаче №2 нейросетью."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54fd1bc7",
      "metadata": {},
      "source": [
        "По результату тестового задания должен быть представлен краткий отчет с описанием выполненных работ, результатов тестирования НС и примерами изображений размеченных нейросетью.\n",
        "\n",
        "Примечание: в данном случае для примера сделана просто визуализация разметки, вы должны будете сделать раскраску по результатам сегментации входных изображений нейросетью.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coCWqLxU2lMo",
      "metadata": {
        "id": "coCWqLxU2lMo"
      },
      "source": [
        "## Подключаем необходимые модули"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179",
      "metadata": {
        "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is OFF\n"
          ]
        }
      ],
      "source": [
        "# испорт основных библиотек\n",
        "import os\n",
        "#import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# импорт спец. библиотек и функций\n",
        "#import pydot\n",
        "#import graphviz\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "from skimage import measure\n",
        "from skimage.io import imread, imsave, imshow\n",
        "from skimage.transform import resize\n",
        "from skimage.filters import gaussian\n",
        "from skimage.morphology import dilation, disk\n",
        "from skimage.draw import polygon, polygon_perimeter\n",
        "\n",
        "# проверка наличия GPU-ускорителя\n",
        "print(f'GPU is {\"ON\" if tf.config.list_physical_devices(\"GPU\") else \"OFF\" }')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3",
      "metadata": {
        "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3"
      },
      "source": [
        "## Подготовим набор данных для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "82be98ae-bc6e-4d49-b205-db5130879caa",
      "metadata": {
        "id": "82be98ae-bc6e-4d49-b205-db5130879caa"
      },
      "outputs": [],
      "source": [
        "CLASSES = 2 # кол-во классов + один класс обозначающий задний план\n",
        "COLORS = ['black', 'red', 'green'] # цветовое обозначение классов\n",
        "\n",
        "SAMPLE_SIZE = (256, 256) # размер входного изображения для НС\n",
        "OUTPUT_SIZE = (2448, 2448) # разммер изображения на выходе НС"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "500caf45-f2b9-48af-8f91-7d31d44ec266",
      "metadata": {
        "id": "500caf45-f2b9-48af-8f91-7d31d44ec266"
      },
      "outputs": [],
      "source": [
        "# функция загрузки и преобразования фото и маски\n",
        "def load_images(image, mask):\n",
        "    image = tf.io.read_file(image) # чтение фото\n",
        "    image = tf.io.decode_jpeg(image)\n",
        "    image = tf.image.resize(image, OUTPUT_SIZE) # стандартизация размера\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = image / 255.0 # нормализация фото (отмасштабировали будущие признаки)\n",
        "\n",
        "    # аналогичная операция выполняется для маски\n",
        "    mask = tf.io.read_file(mask)\n",
        "    mask = tf.io.decode_png(mask)\n",
        "    mask = tf.image.resize(mask, OUTPUT_SIZE)\n",
        "    mask = tf.image.convert_image_dtype(mask, tf.float32)\n",
        "    mask = mask / 128.0\n",
        "    #mask = mask[:, :, 0:1] # отбрасываю Blue channel из-за неинформативности\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e591bcb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# функция аугментация фото и маски, соответственно\n",
        "def augmentate_images(image, masks):\n",
        "    # увеличение масштаба на случайную величину\n",
        "    random_crop = tf.random.uniform((), 0.8, 1)\n",
        "    image = tf.image.central_crop(image, random_crop)\n",
        "    masks = tf.image.central_crop(masks, random_crop)\n",
        "\n",
        "    # отражение по горизонтали\n",
        "    random_flip = tf.random.uniform((), 0, 1)\n",
        "    if random_flip >= 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        masks = tf.image.flip_left_right(masks)\n",
        "\n",
        "    # назначение входного размера фото и маски\n",
        "    image = tf.image.resize(image, SAMPLE_SIZE)\n",
        "    masks = tf.image.resize(masks, SAMPLE_SIZE)\n",
        "\n",
        "    return image, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6dcf6424",
      "metadata": {},
      "outputs": [],
      "source": [
        "# загрузка имён фото и соответствующих масок\n",
        "def get_image_dataset(sample):\n",
        "    images = []\n",
        "    masks  = []\n",
        "    \n",
        "    # загрузка имён фото и соответствующих масок\n",
        "    file = open('fishki_voc_dataset/ImageSets/Segmentation/' + sample + '.txt', 'r')\n",
        "    for line in file:\n",
        "        images.append('fishki_voc_dataset/JPEGImages\\\\' + line[:-1] + '.jpg')\n",
        "        masks.append('fishki_voc_dataset/SegmentationClass\\\\'+ line[:-1] +'.png')\n",
        "    file.close()\n",
        "\n",
        "    # формирование набора данных\n",
        "    images_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "    masks_dataset  = tf.data.Dataset.from_tensor_slices(masks)\n",
        "    dataset = tf.data.Dataset.zip((images_dataset, masks_dataset))\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f6151ff4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# загрузка датасетов\n",
        "train = get_image_dataset('train')\n",
        "train = train.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)       # загрузка данных в память с помощью функции load_images\n",
        "#train = train.repeat(60)                                                  # копирование датасета в памяти N раз\n",
        "train = train.map(augmentate_images, num_parallel_calls=tf.data.AUTOTUNE) # аугментация датасета с помощью функции augmentate_images\n",
        "train = train.batch(16)\n",
        "\n",
        "valid = get_image_dataset('val')\n",
        "valid = valid.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid = valid.batch(16)\n",
        "\n",
        "test = get_image_dataset('test')\n",
        "test = test.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test = test.batch(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "81f1d7ae",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_ParallelMapDataset element_spec=(TensorSpec(shape=(256, 256, None), dtype=tf.float32, name=None), TensorSpec(shape=(256, 256, None), dtype=tf.float32, name=None))>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86faac58",
      "metadata": {},
      "outputs": [],
      "source": [
        "# проверка загрузки данных\n",
        "batch = list(train.batch(3).as_numpy_iterator())\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "c51ae5c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Кол-во объектов:             3\n",
            "Кол-во размерностей объекта: 2\n",
            "Глубина объекта:             5\n"
          ]
        }
      ],
      "source": [
        "print('Кол-во объектов в batch`e:  ', len(batch))\n",
        "print('Кол-во размерностей объекта:', len(batch[0]))\n",
        "print('Глубина объекта:            ', len(batch[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923",
      "metadata": {
        "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923"
      },
      "source": [
        "## Посмотрим на содержимое набора данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2",
      "metadata": {
        "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "images_and_masks = list(train.take(5))\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize=(15, 5), dpi=125)\n",
        "\n",
        "for i, (image, masks) in enumerate(images_and_masks):\n",
        "    ax[0, i].set_title('Image')\n",
        "    ax[0, i].set_axis_off()\n",
        "    ax[0, i].imshow(image)\n",
        "\n",
        "    ax[1, i].set_title('Mask')\n",
        "    ax[1, i].set_axis_off()\n",
        "    ax[1, i].imshow(image)\n",
        "\n",
        "    for channel in range(CLASSES):\n",
        "        contours = measure.find_contours(np.array(masks[:,:,channel]))\n",
        "        for contour in contours:\n",
        "            ax[1, i].plot(contour[:, 1], contour[:, 0], linewidth=1, color=COLORS[channel])\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482b3f41-5324-41e1-944d-809ec06ee959",
      "metadata": {
        "id": "482b3f41-5324-41e1-944d-809ec06ee959"
      },
      "source": [
        "## Обозначим основные блоки модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b",
      "metadata": {
        "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b"
      },
      "outputs": [],
      "source": [
        "def input_layer():\n",
        "    return tf.keras.layers.Input(shape=SAMPLE_SIZE + (3,))\n",
        "\n",
        "\n",
        "def downsample_block(filters, size, batch_norm=True):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    model.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if batch_norm:\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    return model\n",
        "\n",
        "\n",
        "def upsample_block(filters, size, dropout=False):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if dropout:\n",
        "        model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.ReLU())\n",
        "    return model\n",
        "\n",
        "\n",
        "def output_layer(size):\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    return tf.keras.layers.Conv2DTranspose(CLASSES, size, strides=2, padding='same', kernel_initializer=initializer, activation='sigmoid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc",
      "metadata": {
        "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc"
      },
      "source": [
        "## Построим U-NET подобную архитектуру"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c4c326c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "inp_layer = input_layer()\n",
        "\n",
        "downsample_stack = [\n",
        "    downsample_block(64, 4, batch_norm=False),\n",
        "    downsample_block(128, 4),\n",
        "    downsample_block(256, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "    downsample_block(512, 4),\n",
        "]\n",
        "\n",
        "upsample_stack = [\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(512, 4, dropout=True),\n",
        "    upsample_block(256, 4),\n",
        "    upsample_block(128, 4),\n",
        "    upsample_block(64, 4)\n",
        "]\n",
        "\n",
        "out_layer = output_layer(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40",
      "metadata": {
        "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "# добавление skip-connections связей\n",
        "x = inp_layer\n",
        "\n",
        "downsample_skips = []\n",
        "\n",
        "for block in downsample_stack:\n",
        "    x = block(x)\n",
        "    downsample_skips.append(x)\n",
        "\n",
        "downsample_skips = reversed(downsample_skips[:-1])\n",
        "\n",
        "for up_block, down_block in zip(upsample_stack, downsample_skips):\n",
        "    x = up_block(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, down_block])\n",
        "\n",
        "out_layer = out_layer(x)\n",
        "\n",
        "unet_like = tf.keras.Model(inputs=inp_layer, outputs=out_layer)\n",
        "\n",
        "tf.keras.utils.plot_model(unet_like, show_shapes=True, dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67f732f-120b-49a8-bc7c-304709f12db5",
      "metadata": {
        "id": "b67f732f-120b-49a8-bc7c-304709f12db5"
      },
      "source": [
        "## Определим метрики и функции потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381",
      "metadata": {
        "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381"
      },
      "outputs": [],
      "source": [
        "def dice_mc_metric(a, b):\n",
        "    a = tf.unstack(a, axis=3)\n",
        "    b = tf.unstack(b, axis=3)\n",
        "\n",
        "    dice_summ = 0\n",
        "\n",
        "    for i, (aa, bb) in enumerate(zip(a, b)):\n",
        "        numenator = 2 * tf.math.reduce_sum(aa * bb) + 1\n",
        "        denomerator = tf.math.reduce_sum(aa + bb) + 1\n",
        "        dice_summ += numenator / denomerator\n",
        "\n",
        "    avg_dice = dice_summ / CLASSES\n",
        "\n",
        "    return avg_dice\n",
        "\n",
        "def dice_mc_loss(a, b):\n",
        "    return 1 - dice_mc_metric(a, b)\n",
        "\n",
        "def dice_bce_mc_loss(a, b):\n",
        "    return 0.3 * dice_mc_loss(a, b) + tf.keras.losses.binary_crossentropy(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c",
      "metadata": {
        "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c"
      },
      "source": [
        "## Компилируем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1",
      "metadata": {
        "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1"
      },
      "outputs": [],
      "source": [
        "unet_like.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=['iou'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bfd329-31b4-4200-8282-1cb066d52b83",
      "metadata": {
        "id": "75bfd329-31b4-4200-8282-1cb066d52b83"
      },
      "source": [
        "## Обучаем нейронную сеть и сохраняем результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb",
      "metadata": {
        "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1085, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_dice \u001b[39m=\u001b[39m unet_like\u001b[39m.\u001b[39;49mfit(train, validation_data\u001b[39m=\u001b[39;49mvalid, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m#unet_like.save_weights('SemanticSegmentationNetworks/unet_like')\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file0n9d1f1h.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1085, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"D:\\Users\\Dispers\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n"
          ]
        }
      ],
      "source": [
        "history_dice = unet_like.fit(train, validation_data=valid, epochs=2, initial_epoch=0)\n",
        "\n",
        "#unet_like.save_weights('SemanticSegmentationNetworks/unet_like')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311e395e-5465-4507-a75c-7ea95dc19e69",
      "metadata": {
        "id": "311e395e-5465-4507-a75c-7ea95dc19e69"
      },
      "source": [
        "## Загрузим модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c",
      "metadata": {
        "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c"
      },
      "outputs": [],
      "source": [
        "unet_like.load_weights('SemanticSegmentationLesson/networks/unet_like')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e978c692-4136-419b-9365-e5fbf98bbf50",
      "metadata": {
        "id": "e978c692-4136-419b-9365-e5fbf98bbf50"
      },
      "source": [
        "## Проверим работу сети на всех кадрах из видео"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c",
      "metadata": {
        "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c"
      },
      "outputs": [],
      "source": [
        "rgb_colors = [\n",
        "    (0,   0,   0),\n",
        "    (255, 0,   0),\n",
        "    (0,   255, 0),\n",
        "    (0,   0,   255),\n",
        "    (255, 165, 0),\n",
        "    (255, 192, 203),\n",
        "    (0,   255, 255),\n",
        "    (255, 0,   255)\n",
        "]\n",
        "\n",
        "frames = sorted(glob.glob('SemanticSegmentationLesson/videos/original_video/*.jpg'))\n",
        "\n",
        "for filename in frames:\n",
        "    frame = imread(filename)\n",
        "    sample = resize(frame, SAMPLE_SIZE)\n",
        "\n",
        "    predict = unet_like.predict(sample.reshape((1,) +  SAMPLE_SIZE + (3,)))\n",
        "    predict = predict.reshape(SAMPLE_SIZE + (CLASSES,))\n",
        "\n",
        "    scale = frame.shape[0] / SAMPLE_SIZE[0], frame.shape[1] / SAMPLE_SIZE[1]\n",
        "\n",
        "    frame = (frame / 1.5).astype(np.uint8)\n",
        "\n",
        "    for channel in range(1, CLASSES):\n",
        "        contour_overlay = np.zeros((frame.shape[0], frame.shape[1]))\n",
        "        contours = measure.find_contours(np.array(predict[:,:,channel]))\n",
        "\n",
        "        try:\n",
        "            for contour in contours:\n",
        "                rr, cc = polygon_perimeter(contour[:, 0] * scale[0],\n",
        "                                           contour[:, 1] * scale[1],\n",
        "                                           shape=contour_overlay.shape)\n",
        "\n",
        "                contour_overlay[rr, cc] = 1\n",
        "\n",
        "            contour_overlay = dilation(contour_overlay, disk(1))\n",
        "            frame[contour_overlay == 1] = rgb_colors[channel]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    imsave(f'SemanticSegmentationLesson/videos/processed/{os.path.basename(filename)}', frame)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
